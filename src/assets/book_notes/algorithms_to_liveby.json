{
    "Title": "Algorithms to live by: the computer science of human decisions",
    "Author": "Brian Christian, Tom Griffiths",
    "Year": "2016",
    "Notes": [
        {"text": "Explores how algorithms from computer science can be applied to everyday life and covers various topics, including: Optimal stopping, explore/exploit tradeoff, sorting algorithms, caching and memory hierarchy, scheduling and time management techniques, the Bayes' rule, relaxation techniques for problem-solving, randomness and its role in decision-making, evolution, and creativity, networking principles and their application to communication dynamics, and game theory and its insights into strategic interactions."}
    ],
    "Quotes": [ 
        {"text": "Optimal stopping algorithms are designed to address the challenge of making a decision when presented with a sequence of options over time. The goal is to pinpoint the ideal moment to halt the search and commit to a choice. These algorithms are particularly useful in apartment hunting (determining how many apartments to view before selecting one), parking spot hunt (deciding when to accept a parking spot instead of continuing to search for a closer one), hiring (interviewing candidates and selecting the best one from the pool of applicants), dating etc."},
        {"text": "Optimal stopping algorithms strive to maximise the probability of choosing the best possible option, given limitations in time and information. A well-known example, the 37% Rule, suggests that when faced with a sequence of options, the best strategy is to observe the first 37% of the options, then select the first option that surpasses all those observed so far. However, the performance of a specific optimal stopping algorithm hinges on the distribution of the options and the assumptions made about that distribution."},
        {"text": "The Look-Then-Leap Rule involves dedicating a set period of time to exploring options without making a decision. Then, once that period is over, the strategy is to immediately commit to the very next option that is better than all of the options explored during the initial period. Using the Look-Then-Leap Rule, you would spend the first 37% of your time (11 days) just looking at apartments, without the intention of signing a lease. After that period is over, the strategy is to be prepared to sign a lease on the very next apartment that is better than all the apartments you've seen in the first 11 days. This may seem counterintuitive, as it means potentially letting a good apartment slip away. However, mathematically, this is the strategy that gives you the highest chance of finding the best possible apartment in the full month."},
        {"text": "Optimal stopping algorithms offer a structured framework for decision-making in uncertain situations. By incorporating factors like the cost of search and the diminishing returns of continued exploration, they offer a more rational approach compared to relying on intuition or arbitrary rules of thumb. A key limitation of these algorithms lies in their dependence on assumptions about the distribution of options. Real-world scenarios often deviate from idealised models, and unexpected patterns in data can affect the accuracy of the predictions.  Additionally, accurately quantifying factors like the cost of search or the value of different options can be subjective and challenging."},
        {"text": "Explore/exploit algorithms help navigate the dilemma of balancing the need to explore new possibilities with the advantages of exploiting known good options. This challenge is especially pertinent in situations with limited information. A crucial insight is that the time interval in which you will be making decisions influences whether exploration or exploitation is more advantageous."},
        {"text": "Choosing restaurants (deciding between visiting a familiar or trying a new restaurant), choosing between a beloved book and a new title, A/B testing in online campaigns (determining the optimal duration for testing different versions of a website or ad before settling on the best-performing option), research and development (allocating resources between exploring new ideas and exploiting existing technologies)."},
        {"text": "Explore/exploit algorithms strive to maximise long-term rewards by strategically balancing exploration and exploitation. Algorithms like the Upper Confidence Bound assign a value that considers both the estimated reward of an option and the uncertainty associated with it. The Gittins Index offers another approach, calculating a value for each option based on its potential future rewards, discounted over time. The effectiveness of a specific algorithm is determined by factors such as the distribution of rewards and the discount rate applied to future rewards."},
        {"text": "Explore/exploit algorithms facilitate learning and adaptation. By continuously exploring, they enable the discovery of new and potentially superior options, while simultaneously exploiting known good options to maximise immediate rewards. This adaptive nature proves particularly valuable in dynamic environments. The implementation of these algorithms can be computationally intensive.  Calculating confidence bounds or Gittins indices can require significant processing power and memory, especially when dealing with a large number of options. Additionally, accurate estimation of the rewards associated with each option is essential."},
        {"text": "Sorting algorithms are fundamental tools designed to arrange items in a specific order, such as alphabetically, numerically, or chronologically. This ordering process facilitates efficient search, retrieval, and analysis of data. Sorting plays a critical role in a multitude of applications such as in organising data in tables for quick and efficient queries, ordering search results based on relevance or other criteria, presenting data in a user-friendly manner, like arranging contacts in a phone book alphabetically."},
        {"text": "Sorting algorithms are often evaluated by their time complexity, measured in Big-O notation, which represents how the algorithm's runtime scales with the size of the input data. While the fastest sorting algorithms take O(n log n) time, real-world applications often use techniques like bucket sort to achieve linear time sorting (O(n)). Examples include, Mergesort: A divide-and-conquer algorithm with a time complexity of O(n log n), offering consistent performance. Quicksort:  Known for its average-case efficiency of O(n log n) but susceptible to worst-case scenarios that degrade performance to O(n^2). Bubblesort: A simple algorithm with a time complexity of O(n^2), generally less efficient for large datasets."},
        {"text": "Efficient sorting algorithms are paramount for managing and processing large datasets. Well-chosen algorithms dramatically reduce the time needed to search, retrieve, and analyse information. Different sorting algorithms have distinct trade-offs between speed, memory usage, and stability. Choosing the most appropriate algorithm depends heavily on the specific application and the characteristics of the data being sorted."},
        {"text": "Caching algorithms are designed to enhance the speed and efficiency of data retrieval by strategically storing frequently accessed data in a small, fast, temporary storage area known as a cache. This reduces the need to access slower main memory or disk storage. Caching finds applications in diverse areas: Computer systems (speeding up access to frequently used data in processors, hard drives, and web browsers), applications (improving performance by caching frequently accessed data in memory), web servers (caching static content, like images and HTML files, to reduce server load and improve website loading times)."},
        {"text": "Caching algorithms aim to minimise cache misses, which happens when the requested data is not found in the cache, forcing a retrieval from the slower main storage. Commonly used algorithms include Least Recently Used (LRU): Evicts the least recently used data from the cache when it becomes full, prioritising recently accessed information. First In, First Out (FIFO):  Evicts data based on the order it was added to the cache. Least Frequently Used (LFU):  Evicts the least frequently used data."},
        {"text": "Caching can significantly improve system and application performance by reducing latency, the delay in accessing data. By keeping frequently used data readily available, caching minimises the need to access slower storage, leading to faster processing times and smoother user experiences. The effectiveness of caching depends heavily on cache size and the choice of eviction policy. A small cache can lead to frequent cache misses, while a complex eviction policy can introduce computational overhead. Striking a balance between these factors is crucial for optimising cache performance."},
        {"text": "Caching is an essential optimisation technique for contemporary computer systems.  As the gap between processor speeds and memory access times continues to widen, caching plays an increasingly critical role in maintaining system responsiveness and enabling efficient data processing."},
        {"text": "Game theory algorithms are designed to analyse strategic interactions between rational agents, where the outcome for each agent depends on the choices made by others. They help decipher the dynamics of situations involving competition and cooperation. Game theory is applied across various domains: Auctions (predicting bidding behaviour and designing auction mechanisms that maximise revenue or social welfare),  Negotiations (understanding the strategies of negotiating parties and identifying potential compromises or solutions), market competition (modeling the behaviour of firms in a market and predicting market outcomes), social interactions (explaining and predicting behaviour in social dilemmas, such as the Prisoner's Dilemma)."},
        {"text": "Game theory algorithms aim to identify equilibrium outcomes, situations where no player can improve their payoff by unilaterally changing their strategy.  A widely known example is the Nash equilibrium, which describes a state where no player has an incentive to deviate from their chosen strategy, given the strategies of the other players.  Game theory offers a formal framework for understanding strategic decision-making. It helps predict how rational agents will behave in situations where their payoffs are intertwined, providing insights into the dynamics of competition, cooperation, and conflict (checkout Thinking Strategically notes)."},
        {"text": "When it comes to scheduling tasks, there are several optimal strategies, each catering to specific goals and constraints. Before diving into these strategies, it's crucial to explicitly define your goals and metrics for success. Only then can you select the most effective approach for your situation."},
        {"text": "Scheduling  by due date: If minimising the lateness of tasks is the goal, prioritise them by due date using the Earliest Due Date algorithm. This method, often used instinctively, involves tackling tasks with the nearest deadlines first. It's optimal for reducing maximum lateness but might not be suitable if other factors, like task importance or minimising the number of late tasks, are more critical."},
        {"text": "Moore's Algorithm: When the number of late tasks is a concern, Moore's Algorithm suggests a different approach. It focuses on tasks with the earliest due dates and ensures their timely completion. Tasks that can't be finished on time are addressed later in any order. This algorithm aims to minimise the number of late tasks, which can be particularly useful in scenarios where completing everything on time is challenging."},
        {"text": "Shortest Processing Time: If minimising the total time spent on tasks is the priority, the Shortest Processing Time algorithm is most effective. This approach advocates for completing shorter tasks first, creating a sense of accomplishment and momentum. It's beneficial for maintaining motivation and efficiency, especially when dealing with many small tasks."},
        {"text": "Preemption: In unpredictable situations where new tasks can arrive at any time, preemption, or interrupting a task to work on a higher-priority one, can be beneficial. For instance, if a new task arrives with a due date earlier than the current task, it might be necessary to switch gears and prioritise the more urgent task. This allows for flexibility and responsiveness in dynamic environments."},
        {"text": "Interrupt coalescing: To mitigate the negative impact of context switching, interrupt coalescing, or grouping interruptions together, is an effective strategy. Regularly scheduled meetings, office hours, or designating specific times for checking emails are examples of this technique. It minimises distractions and allows for longer periods of focused work, improving overall productivity."},
        {"text": "These optimal strategies, drawn from the principles of scheduling theory, provide a framework for approaching task management with greater efficiency and effectiveness. Remember to tailor your chosen strategy to your specific goals, "},
        {"text": "Forward and Backward Reasoning: Bayes' Rule is a fundamental concept in probability and statistics that helps refine predictions by incorporating new evidence. It involves reasoning forward and backward, starting with prior beliefs, incorporating new evidence, and then updating those beliefs to create more accurate predictions."},
        {"text": "The Bayes' Rule is described as a mathematical formula that explains how to combine preexisting beliefs with newly observed evidence to update predictions. Bayes's Rule requires some preexisting beliefs, or priors, even if they are just a guess."},
        {"text": "The Multiplicative Rule is used for making predictions about data that follows a power-law distribution. The longer something has gone on, the longer it is expected to continue. An example is the Copernican Principle, which says the lifespan of something will be double its current age. For example, if a business has been in operation for five years, it is predicted to continue for five more years. Surprising events are more surprising the longer someone has been waiting for them. For example, it would be more surprising if a 100-year-old company went bankrupt than if a five-year-old company did."},
        {"text": "The Average Rule is used for making predictions about data that follows a normal distribution. To apply the Average Rule, take the average of the existing data. An example is predicting the runtime of a movie, which usually falls somewhere between 90 minutes and three hours. Surprising events are more surprising if they deviate farther from the average. For example, it would be more surprising for a movie to run for five hours than for a movie to run for two and a half hours."},
        {"text": "The Additive Rule is used for making predictions about data that follows an Erlang distribution. The same prediction is made regardless of the amount of time that has passed. An example is someone saying they will be ready to leave in five minutes, and then continuing to say that they will be ready in five minutes regardless of how much time passes. This is because the prediction is based on the amount of time the person expects a task to take rather than the amount of time that has already passed. Surprising events are equally surprising regardless of how much time has passed. For example, it would be equally surprising if the person in the previous example took an hour to get ready regardless of whether they had already been getting ready for five minutes or 55 minutes."},
        {"text": "Focus on Details vs. the Bigger Picture: Overfitting occurs when we focus too much on specific details while neglecting the bigger picture. It often leads to flawed judgments as the emphasis shifts from general patterns to idiosyncrasies. In machine learning, for example, an overfitted model might perform well on the training data but poorly on new data because it has learned the noise in the training data rather than the underlying pattern."},
        {"text": "Simplifying Problems: Relaxation techniques simplify complex problems by transforming constraints into penalties. This makes them more manageable and allows for finding solutions that might not be perfect but are still acceptable."},
        {"text": "Constraint Relaxation: This involves removing some of the constraints of a problem to make it easier to solve.  Once a solution is found for the simplified problem, the removed constraints are added back in, and the solution is adapted accordingly. An example is the traveling salesman problem, where the goal is to find the shortest route that visits a set of cities exactly once and returns to the starting city. This problem is computationally complex. However, if the constraint of visiting each city only once is removed, it becomes much easier to find a solution. This solution can then be used as a starting point for finding a good solution to the original traveling salesman problem. Constraint Relaxation can be thought of as approaching a problem by first asking, 'What would I do if I wasn't afraid?' or 'What would I do if I could not fail?' This allows for creative problem-solving by temporarily setting aside limiting factors."},
        {"text": "Continuous Relaxation: This method converts a problem with discrete variables (like integers) into one with continuous variables (like real numbers), facilitating easier solution-finding. This technique is often applied in scheduling and route optimisation. An example is the problem of deciding which friends to invite to a party given a limited number of invitations. Continuous Relaxation would allow for sending out fractional invitations. Then, to arrive at a solution to the original problem, anyone who received half of an invitation or more in the relaxed solution would be invited to the party. Continuous Relaxation can be applied to situations where making an all-or-nothing choice is difficult. By considering a spectrum of possibilities, it might become clearer which option is preferable."},
        {"text": "Lagrangian Relaxation: This approach converts impossibilities into penalties, enabling progress in challenging situations. By assigning costs to violations of constraints, it transforms initially unsolvable problems into solvable ones. An example is scheduling a sports league where teams have preferences about which other teams they play and when.  Lagrangian Relaxation could be used to allow for some less-preferred matchups if those matchups make it easier to satisfy other constraints, such as minimising travel time. Lagrangian Relaxation is similar to the idea that while you technically have the choice to break any rule, you must also accept the consequences. By factoring in the costs of violating certain constraints, it becomes possible to find solutions that might not have been considered otherwise."},
        {"text": "The goal of relaxation techniques is not to find perfect solutions, but rather to find good solutions efficiently. Relaxation can also provide insights into the nature of a problem and the trade-offs involved in finding a solution. By understanding these techniques, people can apply them to a variety of real-world problems, even if they are not consciously aware of the underlying computational concepts."},
        {"text": "Exploring New Possibilities: Randomness is essential for exploring new possibilities and breaking free from local optima in decision-making. By introducing chance, it allows for discovering solutions that might be overlooked by purely deterministic approaches. This is particularly valuable when dealing with complex problems where the optimal solution is difficult to find directly."},
        {"text": "Randomised algorithms, which utilise random numbers, can provide faster and more efficient solutions to complex problems than deterministic algorithms. They offer a trade-off between speed and certainty, acknowledging that accepting some error probability can lead to quicker solutions. This is especially beneficial when time is a critical factor."},
        {"text": "Evolution and Creativity: In evolution, random mutations drive the exploration of new traits, enabling adaptation to changing environments. Similarly, in creative processes, introducing randomness can foster novel ideas and solutions. This is why techniques like brainstorming often encourage participants to think outside the box and generate ideas without judgment, even if they seem unusual or unconventional."},
        {"text": "In networking, randomness plays a vital role in breaking symmetry and avoiding collisions. For instance, in wireless networks, if two devices try to transmit data simultaneously, a collision occurs, corrupting both transmissions. By introducing random elements into communication protocols, it's ensured that devices can share resources efficiently without interference."},
        {"text": "TCP/IP Example: In TCP/IP networks, random sequence numbers help identify lost packets and maintain reliable data transmission. This use of randomness is essential for the robustness and reliability of the internet, as it helps to overcome the inherent unreliability of the underlying physical networks."},
        {"text": "Laplace's Law, developed by French mathematician Pierre-Simon Laplace, provides a straightforward method for estimating probabilities when facing limited data. It states that the expected probability of an event is equal to the number of times it has occurred plus one, divided by the number of opportunities plus two.  This can be represented by the formula: (w + 1) / (n + 2), where: w represents the number of times the event has happened, n represents the total number of attempts or opportunities for the event to occur."},
        {"text": "This simple yet powerful formula works effectively regardless of the amount of data available. Consider a few examples:  If a bus has been late 3 times out of 7 trips, the estimated probability of it being late on the next trip, according to Laplace's Law, would be (3 + 1) / (7 + 2) = 4/9 or approximately 44%. Predicting a softball team's win:  If a team has won 6 out of 10 games, the estimated probability of winning their next game would be (6 + 1) / (10 + 2) = 7/12 or approximately 58%. The sunrise problem: This classic example demonstrates Laplace's Law's effectiveness with vast datasets. Given that the sun has risen approximately 1.6 trillion days in a row, the probability of it rising tomorrow is (1.6 trillion + 1) / (1.6 trillion + 2), which is virtually indistinguishable from 100%."},
        {"text": "Laplace's Law offers a practical and reliable method for estimating probabilities, especially when navigating real-world scenarios where data might be limited. It provides a valuable tool for making informed decisions and predictions even when faced with uncertainty."},
        {"text": "Networking principles, initially developed for machine-to-machine communication, can offer insightful perspectives on human communication and social dynamics.  The core concepts of protocols, packet switching, acknowledgments, flow control, and Exponential Backoff, while designed for the internet, find surprising resonance in the way people interact and form connections."},
        {"text": "Similar to how computers rely on shared protocols to communicate effectively, human connection is built upon a foundation of shared conventions and expectations. These protocols, ranging from basic greetings to complex social norms, establish a common ground for understanding and interaction."},
        {"text": "Without established protocols, communication becomes prone to misunderstandings and breakdowns. Even the seemingly simple act of initiating a conversation requires a shared understanding of appropriate procedures. For instance, knowing who should call whom for a scheduled phone call or interpreting the appropriate length of silence before assuming a message wasn't received, all rely on established protocols."},
        {"text": "Anxiety often arises from the uncertainty of whether the other party has received and understood the message. Just as TCP uses a triple handshake to ensure a reliable connection, humans employ verbal and nonverbal cues, such as uh-huhs and nods, to confirm reception and encourage continued communication."},
        {"text": "The internet's reliance on packet switching, where messages are broken down into smaller packets and transmitted independently, mirrors the dynamic nature of human communication.  Rather than relying on a continuous, dedicated channel like a phone call, people exchange information in a more fragmented and asynchronous manner, especially in written communication."},
        {"text": "This packet-based approach offers flexibility and robustness. Just as packets can be rerouted through different paths on the internet to reach their destination, individuals can adapt their communication style and choose different channels depending on the situation."},
        {"text": "The concept of acknowledgment packets (ACKs) in TCP, which confirm the successful delivery of data packets, underscores the importance of feedback in human communication. Providing feedback is not merely a matter of politeness but is crucial for regulating the flow of information and ensuring mutual understanding."},
        {"text": "The lack of timely acknowledgments can lead to communication breakdowns. Just as a computer will slow down its transmission rate if it doesn't receive ACKs, humans tend to reduce their communication frequency or level of detail if they sense a lack of engagement from the other party."},
        {"text": "TCP's flow control mechanisms, which adjust transmission rates to avoid network congestion, highlight the importance of managing the pace of interaction in human communication.  Just as an overloaded network can lead to dropped packets and slowdowns, overwhelming someone with information or demands can lead to miscommunication, frustration, and strained relationships."},
        {"text": "The TCP principle of Additive Increase, Multiplicative Decrease (AIMD), governs how senders adjust their transmission rates based on feedback from the network. This principle suggests a strategy of gradually increasing communication frequency or intensity until there's a sign of overload, and then sharply decreasing it before gradually increasing again. This can be applied to various social interactions, from building relationships to negotiating agreements."},
        {"text": "The problem of buffer bloat is excessive buffering in a network that can lead to delays and reduced responsiveness. This phenomenon has implications for human communication as well, particularly in the context of modern, always-connected technologies. The constant influx of information and demands can create a sense of overwhelm and make it difficult to prioritize and respond effectively."},
        {"text": "The Exponential Backoff algorithm, used in networking to handle collisions and unreliable connections, offers a strategy for dealing with unreliability in human relationships. This algorithm dictates that when a transmission fails, the sender waits a random amount of time before trying again, with the waiting time increasing exponentially with each failed attempt."},
        {"text": "This approach can be applied to situations where persistence is necessary but constant attempts at communication can be counterproductive. For example, when trying to reconnect with someone who is unresponsive, repeatedly reaching out without giving them space might push them further away. Exponential Backoff suggests a more measured approach, gradually increasing the time between attempts while remaining open to re-establishing connection."},
        {"text": "Understanding the principles of networking, even at a conceptual level, provides valuable insights into human communication and social dynamics. These principles highlight the importance of clear protocols, feedback mechanisms, flow control, and strategies for dealing with unreliability, all of which are essential for building strong and sustainable connections."}

    ]
}